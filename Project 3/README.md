# Ανάπτυξη λογισμικού για αλγοριθμικά προβλήματα - Εργασία 3

1115202000232- Μιχαήλ Βιταντζάκης

1115202000151 - Μιχαήλ Χρήστος Ναβάρο Αμαργιανός

## Οργάνωση αρχείων

Στο κεντρικό φάκελο της εργασίας υπάρχουν τα datasets, τα dataset αφού πρώτα έχει μειωθεί η διάστασή τους και ένα γενικό Makefile.

Τα αρχεία κώδικα είναι οργανωμένα σε έξι φακέλους:

- Autoencoder/: περιέχει την υλοποιήση το CNN Autoencoder
- Utils/: περιέχει βοηθητικές συναρτήσεις
- LSH/: περιέχει την υλοποίηση του LSH
- Hypercube/: περιέχει την υλοποίηση του Hypercube
- Graph/: περιέχει την υλοποίηση του ANN με την μέθοδο GNNS και την μέθοδο MRNG
- Cluster/: περιέχει την υλοποίηση του Kmeans

## Μεταγλώττιση

Το γενικό Makefile μπορεί να κάνει compile όλα τα αρχεία κώδικα της εργασίας με την εντολή `make all` και να τα διαγράψει με την εντολή `make clean`.

## Εκτέλεση

Στους φακέλους Cluster και Graph υπάρχουν οι αλλαγμένες main functions των προηγούμενων εργασιών ώστε να παίρνουν ένα επιπλέον ορίσματα για τα επιπλέον redused αρχεία.

Συγκεκριμμένα για το CLuster το επιπλέον όρισμα είναι το `-ri` για το αρχείο εισόδου μειωμένων διαστάσεων και για το Graph το `-rd` και το `-rq` για το αρχείο εισόδου και το αρχείο ερωτήσεων μειωμένων διαστάσεων αντίστοιχα.

## Utilities

### utils

Γενικές συναρτήσεις όπως διάβασμα αρχείου, απόσταση μεταξύ vectors και δημιουργία τυχαίου vector.

Επίσης, περιέχει την υλοποίση της βασικής class για την επίλυση KNN και range search.

## Autoencoder

Χρησιμοποιείτε ένα CNN Autoencoder για την μείωση της διάστασης των δεδομένων ωστε να χρησιμοποιηθεί για K-means και clustering.

### Encoder

Για το encode χρησιμοποιήθηκαν τα layers : Conv2D x2, MaxPooling2D x2, Flatten, Dropout x2, Dense.

### Decoder

Για το decode χρησιμοποιήθηκαν τα layers :  Dense, Dropout x2 , Reshape, Conv2DTranspose x2, UpSampling2D.

### Hyperparameters

#### Dropout rate

Χρησιμοποιούμε dropout rate για την αντιμετώπιση του overfitting. Κατά την εκπαίδευση του NN, το dropout εφαρμόζει τυχαία απενεργοποιήσεις ενός ποσοστού των νευρώνων σε κάθε επίπεδο του δικτύου κατά τη διάρκεια κάθε εποχής της εκπαίδευσης.

#### Learning rate

Χρησιμοποιύμε learning rate για να καθορίσουμε το πόσο γρήγορα θα ενημερώνονται τα βάρη του μοντέλου κατά την εκπαίδευση.

#### Batch size

Χρησιμοποιήθηκε batch size 64 ώστε να είναι αρκετά μεγάλο ώστε να μην χρειάζεται πολύς χρόνος για την εκπαίδευση του μοντέλου και αρκετά μικρό ώστε να μην χρειάζεται πολύ μνήμη.

#### Epochs

Χρησιμοποιήθηκαν 5 εποχές για την εκπαίδευση του μοντέλου καθώς μετά από αυτό το σημείο δεν παρατηρούνται σημαντικές βελτιώσεις στο κόστος.

#### Optimizer

Χρησιμοποιήθηκε ο Adam optimizer.

#### Activation function

Στα τελευταία layers χρησιμοποιήθηκε η συνάρτηση ενεργοποίησης sigmoid για να περιοριστούν οι τιμές των pixels στο διάστημα [0,1] (τα pixels είναι κανονικοποιημένα ώστε να γίνει καλύτερη εκπαίδευση του μοντέλου).

Στα υπόλοιπα layers χρησιμοποιήθηκε η συνάρτηση ενεργοποίησης ΕLU επειδή είναι αρκετά απλή και έχει καλή απόδοση.

### Hyperparameter Tuning

Για το Tuning των υπερπαραμέτρων learning rate, dropout rate χρησιμοποιήθηκε η βιβλιοθήκη optuna. Συγκεκριμένα χωρίστηκε το train set σε 90% train και 10% validation και χρησιμοποιήθηκε το MSE του validation set για την επιλογή των υπερπαραμέτρων.

Οι υπόλοιπες υπερπαράμετροι επιλέχθηκαν μετά από δοκιμές.

## B

Για τα παρακάτω αποτελέσματα χρησιμοποιήθηκαν τα πρώτα 10,000 vectors των αρχείων input.dat, reduced_input.dat και τα πρώτα 1,000 vectors των query.dat, reduced_query.dat

- MeanAF: Mean Approximate Factor, προστέθηκε ώστε να μην επηρεάζεται το MAF από κακές περιπτώσεις
- tAverageApproximate: Ο μέσος χρόνος που χρειάζεται το approximate method για να βρει τους γείτονες
- tAverageTrue: Ο μέσος χρόνος που χρειάζεται το brute force method για να βρει τους γείτονες

### Αποτελέσματα αναζήτησης στον παλίο χώρο ( dimension : 784 )

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 1 | 928.92 | 957.38 |
| GNNS | 1.66552 | 11.02 | 946.63 |
| MRNG | 1.87651 | 93.49 | 1023.99 |
| LSH  | 0.0885361 | 516.98 | 915.81 |
| Hypercube | 0.215105 | 56.07 | 1021.99 |

### Αποτελέσματα στον reduced χώρο

#### Latend dimension: 10

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 0.0481524 | 25.18 | 54.81 |
| GNNS | 1.20436 | 5.92 | 71.84 |
| MRNG | 1.40584 | 3.52 | 34.41 |

#### Latend dimension: 50

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 0.474887 | 68.53 | 127.25 |
| GNNS | 1.0408 | 4.04 | 112.18 |
| MRNG | 1.0892 | 8.18 | 109.36 |

#### Latend dimension: 75

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 0.594722 | 107.89 | 180.14 |
| GNNS | 1.11521 | 3.77 | 136.43 |
| MRNG | 1.19197 | 8.89 | 144.11 |

#### Latend dimension: 100

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 0.608417 | 143.49 | 240.21 |
| GNNS | 1.0023 | 4.2 | 209.72 |
| MRNG | 1.06272 | 11.19 | 199.35 |

#### Latend dimension: 150

| Method | MeanAF | tAverageApproximate | tAverageTrue |
|--------|--------|---------------------|--------------|
| Εξαντλητική αναζήτηση | 0.735308 | 244.87 | 307.37 |
| GNNS | 1.06638 | 6.17 | 299.36 |
| MRNG | 1.09342 | 5.19 | 287.21 |


Στην εξαντλητική αναζήτηση παρατηρούμε πως υπάρχει ελάχιστη μεταβολή ανάμεσα στον χρόνο υπολογισμού του προσεγγιστικού γείτονα και του πραγματικού. Ωστόσο, όσο αυξάνεται το latend dimension τόσο αυξάνεται και το MeanAF και ο χρόνος υπολογισμού του προσεγγιστικού γείτονα και πραγματικού γείτονα.

Επιπλέον, παρατηρούμε στις μεθόδους GNNS και MRNG η χρήση του reduced χώρου μειώνει σημαντικά τον χρόνο υπολογισμού του προσεγγιστικού γείτονα. πως όσο αυξάνουμε το latend dimension τόσο μειώνεται το MeanAF και ο χρόνος υπολογισμού του προσεγγιστικού γείτονα. Στην περίπτωση των μεθόδων αυτών όμως, η αύξηση του latend dimension δεν επηρεάζει το MeanA, καθώς παραμεένει της τάξης 1.00 - 1.4, ούτε τον χρόνο υπολογισμού του προσεγγιστικού γείτονα, επηρεάζει μόνο τον χρόνο υπολογισμού του πραγματικού γείτονα, ο οποίος αυξάνεται ανάλογα.

## C

Η μέθοδος που επιλέχθηκε για τον υπολογισμό του kmeans είναι ο αλγόριθμος Lloyd's καθώς έβγαζε καλύτερα και πιο consistent αποτελέσματα από τους προσεγγιστικούς αλγορίθμους.

Για την αξιολόγηση κάθε αριθμού διάστασης χρησιμοποιήθηκε ο μέσος όρος των Silhouette των clusters, ο χρόνος που χρειάζεται ο αλγόριθμος και η τιμή του objective function.

| Dimension | Average silhouette | Objective function | Clustering time |
|-----------|--------------------|--------------------|-----------------|
| 784 | 0.0416855 | 1.97278e+07 |  3.542 |
| 10 | -0.00474967 | 2.16968e+07 |  0.029 |
| 50 | -0.00436616 | 2.16731e+07 |  0.224 |
| 75 | -0.0025636 | 2.16474e+07 |  0.466 |
| 100 | -0.0263332 | 2.17127e+07 |  0.489 |
| 150 | -0.00434784 | 2.16654e+07 | 1.469 |

Εϊναι φανερό ότι όσο αυξάνετε η διάσταση αυξάνετε συμαντικά ο χρόνος. Επίσης, παρατηρούμε πως οι τιμές του objective function είναι παρόμοιες για όλες τις διαστάσεις. Η Silhouette είναι αρνητική για τις μειωμένες διαστάσεις όμως είναι πολύ κοντά στο μηδέν και δεν είναι πολυ μικρότερες από την αντίστοιχη της αρχικής διάστασης. Αξιοσημείοτο είναι ότι ακόμα και για διάσταση 10 τα αποτελέσματα είναι αρκετά καλά ενώ ο χρόνος είναι αρκετά καλύτερος από την αρχική διάσταση.
